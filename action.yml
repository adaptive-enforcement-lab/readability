name: 'Readability'
description: 'Analyze documentation readability and content quality metrics'
author: 'Adaptive Enforcement Lab'

branding:
  icon: 'book-open'
  color: 'blue'

inputs:
  path:
    description: 'Path to analyze (file or directory)'
    required: false
    default: 'docs/'
  format:
    description: 'Output format (table, markdown, json, summary, report)'
    required: false
    default: 'markdown'
  config:
    description: 'Path to config file'
    required: false
    default: ''
  check:
    description: 'Enable check mode (fail on threshold violations)'
    required: false
    default: 'false'
  max-grade:
    description: 'Maximum Flesch-Kincaid grade level'
    required: false
    default: ''
  max-ari:
    description: 'Maximum ARI score'
    required: false
    default: ''
  max-lines:
    description: 'Maximum lines per file'
    required: false
    default: ''
  summary:
    description: 'Write formatted report to job summary (default: true)'
    required: false
    default: 'true'
  summary-title:
    description: 'Title for the job summary section'
    required: false
    default: 'Documentation Readability Report'

outputs:
  report:
    description: 'Analysis report in JSON format'
    value: ${{ steps.analyze.outputs.report }}
  passed:
    description: 'Whether all thresholds were met (true/false)'
    value: ${{ steps.analyze.outputs.passed }}
  files-analyzed:
    description: 'Number of files analyzed'
    value: ${{ steps.analyze.outputs.files-analyzed }}

runs:
  using: 'composite'
  steps:
    - name: Setup Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.23'
        cache: false

    - name: Build readability
      shell: bash
      run: |
        cd ${{ github.action_path }}
        go build -o readability ./cmd/readability

    - name: Run analysis
      id: analyze
      shell: bash
      run: |
        ARGS=""

        # Auto-detect config file if not specified
        if [ -n "${{ inputs.config }}" ]; then
          ARGS="$ARGS --config ${{ inputs.config }}"
        elif [ -f ".readability.yml" ]; then
          ARGS="$ARGS --config .readability.yml"
        fi

        # Always use JSON for capturing outputs, then convert for display
        ARGS="$ARGS --format json"

        if [ "${{ inputs.check }}" = "true" ]; then
          ARGS="$ARGS --check"
        fi

        if [ -n "${{ inputs.max-grade }}" ]; then
          ARGS="$ARGS --max-grade ${{ inputs.max-grade }}"
        fi

        if [ -n "${{ inputs.max-ari }}" ]; then
          ARGS="$ARGS --max-ari ${{ inputs.max-ari }}"
        fi

        if [ -n "${{ inputs.max-lines }}" ]; then
          ARGS="$ARGS --max-lines ${{ inputs.max-lines }}"
        fi

        # Run analysis and capture output
        set +e
        OUTPUT=$(${{ github.action_path }}/readability $ARGS ${{ inputs.path }} 2>&1)
        EXIT_CODE=$?
        set -e

        # Parse JSON output for metrics
        FILES_ANALYZED=$(echo "$OUTPUT" | jq -r 'if type == "array" then length else 0 end' 2>/dev/null || echo "0")
        FAILED_COUNT=$(echo "$OUTPUT" | jq -r '[.[] | select(.status == "fail")] | length' 2>/dev/null || echo "0")

        if [ "$FAILED_COUNT" -eq 0 ]; then
          PASSED="true"
        else
          PASSED="false"
        fi

        # Set outputs
        echo "files-analyzed=$FILES_ANALYZED" >> "$GITHUB_OUTPUT"
        echo "passed=$PASSED" >> "$GITHUB_OUTPUT"

        # Store full report
        {
          echo "report<<EOF"
          echo "$OUTPUT"
          echo "EOF"
        } >> "$GITHUB_OUTPUT"

        # Generate formatted markdown table
        MARKDOWN_TABLE=$(echo "$OUTPUT" | jq -r '
          "| File | Grade | ARI | Fog | Ease | Status |",
          "|------|-------|-----|-----|------|--------|",
          (.[] | "| \(.file) | \(.readability.flesch_kincaid_grade | . * 10 | round / 10) | \(.readability.ari | . * 10 | round / 10) | \(.readability.gunning_fog | . * 10 | round / 10) | \(.readability.flesch_reading_ease | . * 10 | round / 10) | \(.status) |")
        ' 2>/dev/null || echo "$OUTPUT")

        # Write to job summary if enabled
        if [ "${{ inputs.summary }}" = "true" ] && [ -n "$GITHUB_STEP_SUMMARY" ]; then
          {
            echo "## ${{ inputs.summary-title }}"
            echo ""
            echo "$MARKDOWN_TABLE"
            echo ""
            echo "_Files analyzed: ${FILES_ANALYZED} | Passed: $((FILES_ANALYZED - FAILED_COUNT)) | Failed: ${FAILED_COUNT}_"
          } >> "$GITHUB_STEP_SUMMARY"
        fi

        # Generate formatted output for display (stdout)
        FORMAT="${{ inputs.format }}"
        case "$FORMAT" in
          json)
            echo "$OUTPUT"
            ;;
          markdown|report)
            echo "$MARKDOWN_TABLE"
            ;;
          summary)
            echo "$OUTPUT" | jq -r '
              "Files analyzed: \(length)",
              "Passed: \([.[] | select(.status == "pass")] | length)",
              "Failed: \([.[] | select(.status == "fail")] | length)"
            ' 2>/dev/null || echo "$OUTPUT"
            ;;
          *)
            echo "$OUTPUT" | jq -r '.[] | "\(.file): Grade=\(.readability.flesch_kincaid_grade | . * 10 | round / 10), Status=\(.status)"' 2>/dev/null || echo "$OUTPUT"
            ;;
        esac

        # Exit with original code to respect --check
        exit $EXIT_CODE
